{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZfHnvsZZV4q"
   },
   "source": [
    "# URL Scrap"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# install requirements\n",
    "\n",
    "!pip install scrapy\n",
    "!pip install dateparser\n",
    "!pip install furl\n",
    "!pip install scrapy_random_fake_ua\n",
    "!pip install fake_useragent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgxPAkBsZV4v"
   },
   "source": [
    "Scrap URL Today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ANY4xWQ1ZZCs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== START ===============\n",
      "22-May-2022-09:51:56 [CRAWLER] [U] Starting URL scrap\n",
      "2022-05-22 09:51:58 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-22 09:51:58 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 18.9.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.13.0-1025-gcp-x86_64-with-glibc2.29\n",
      "2022-05-22 09:51:58 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'tutorial',\n",
      " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
      " 'LOG_LEVEL': 'INFO',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'RETRY_HTTP_CODES': [429],\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "2022-05-22 09:51:58 [scrapy.extensions.telnet] INFO: Telnet Password: 74aab0c5362df190\n",
      "2022-05-22 09:51:58 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-22 09:51:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy_random_fake_ua.middleware.RandomUserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'tutorial.middlewares.TooManyRequestsRetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-22 09:51:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-22 09:51:59 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-22 09:51:59 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-22 09:51:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-22 09:51:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-22 09:52:04 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-22 09:52:04 [scrapy.extensions.feedexport] INFO: Stored json feed (2052 items) in: ./url/daily/2022-05-22.json\n",
      "2022-05-22 09:52:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 105951,\n",
      " 'downloader/request_count': 171,\n",
      " 'downloader/request_method_count/GET': 171,\n",
      " 'downloader/response_bytes': 1160817,\n",
      " 'downloader/response_count': 171,\n",
      " 'downloader/response_status_count/200': 171,\n",
      " 'elapsed_time_seconds': 5.186928,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 22, 9, 52, 4, 401272),\n",
      " 'httpcompression/response_bytes': 7238707,\n",
      " 'httpcompression/response_count': 171,\n",
      " 'item_scraped_count': 2052,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 64512000,\n",
      " 'memusage/startup': 64512000,\n",
      " 'response_received_count': 171,\n",
      " 'scheduler/dequeued': 171,\n",
      " 'scheduler/dequeued/memory': 171,\n",
      " 'scheduler/enqueued': 171,\n",
      " 'scheduler/enqueued/memory': 171,\n",
      " 'start_time': datetime.datetime(2022, 5, 22, 9, 51, 59, 214344)}\n",
      "2022-05-22 09:52:04 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "22-May-2022-09:52:04 [CRAWLER] [U] Finished URL scrap\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "!echo =============== START =============== | tee -a \"./logs/{today}.log\"\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [U] Starting URL scrap | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "!scrapy crawl url_spider -O ./url/daily/{today}.json 2>&1 3>&1 | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [U] Finished URL scrap | tee -a \"./logs/{today}.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_L9TEV5ZV4y"
   },
   "source": [
    "Merge Daily URL into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1652351437604,
     "user": {
      "displayName": "Zhiyuan DING",
      "userId": "11658084244219590831"
     },
     "user_tz": -60
    },
    "id": "nicA5u0vZV4z",
    "outputId": "5482f725-01b4-408e-eccd-115693763389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== URLCB ===============\n",
      "22-May-2022-09:52:05 [CRAWLER] [U] Found 2052 URLs, added 187 URLs, duplicated 1865 URLs\n"
     ]
    }
   ],
   "source": [
    "!echo =============== URLCB =============== | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create empty JSONArray if it doesn't exist\n",
    "url_db_file = open('./url/url_db.json', 'r' if os.path.exists('./url/url_db.json') else 'w+')\n",
    "daily_file = open('./url/daily/' + today + '.json')\n",
    "\n",
    "daily_json = json.load(daily_file)\n",
    "url_db_json = json.load(url_db_file) if os.stat(\"./url/url_db.json\").st_size > 2 else []\n",
    "\n",
    "url_id_list = [item['id'] for item in url_db_json]\n",
    "\n",
    "# Read daily file and append to url_db_json \n",
    "found = len(daily_json)\n",
    "added = 0\n",
    "for item in daily_json:\n",
    "    if item['id'] not in url_id_list:\n",
    "        url_db_json.append(item)\n",
    "        url_id_list.append(item['id'])\n",
    "        added = added + 1\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [U] Total {len(url_db_json)}, Found {found}, added {added}, duplicated {found-added} URLs | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "# Write result file\n",
    "with open('./url/url_db.json', \"w\") as url_db_file:\n",
    "    url_db_file.write('[' +\n",
    "            ',\\n'.join(json.dumps(i) for i in url_db_json) +\n",
    "            ']\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Q6QaUkxZV40"
   },
   "source": [
    "# Scrap Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4655318,
     "status": "ok",
     "timestamp": 1652362243887,
     "user": {
      "displayName": "Zhiyuan DING",
      "userId": "11658084244219590831"
     },
     "user_tz": -60
    },
    "id": "OSRKVtkGZV40",
    "outputId": "5b7495f5-1f5c-4d8e-a149-e1e56c5f04db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== PROJC ===============\n",
      "22-May-2022-09:52:05 [CRAWLER] [P] Starting project crawler\n",
      "2022-05-22 09:52:06 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-22 09:52:06 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 18.9.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.13.0-1025-gcp-x86_64-with-glibc2.29\n",
      "2022-05-22 09:52:06 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_MAX_DELAY': 5,\n",
      " 'AUTOTHROTTLE_START_DELAY': 0.2,\n",
      " 'AUTOTHROTTLE_TARGET_CONCURRENCY': 0.7,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
      " 'LOG_LEVEL': 'INFO',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'RETRY_HTTP_CODES': [429],\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "2022-05-22 09:52:06 [scrapy.extensions.telnet] INFO: Telnet Password: 34b4fe0517580798\n",
      "2022-05-22 09:52:06 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.spiderstate.SpiderState']\n",
      "2022-05-22 09:52:06 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy_random_fake_ua.middleware.RandomUserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'tutorial.middlewares.TooManyRequestsRetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-22 09:52:06 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-22 09:52:06 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-22 09:52:06 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-22 09:52:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-22 09:52:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-22 09:52:07 [py.warnings] WARNING: /home/stiven_ding144/.local/lib/python3.8/site-packages/dateparser/date_parser.py:35: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n",
      "\n",
      "2022-05-22 09:53:12 [scrapy.extensions.logstats] INFO: Crawled 239 pages (at 239 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-22 09:54:46 [scrapy.extensions.logstats] INFO: Crawled 464 pages (at 225 pages/min), scraped 7 items (at 7 items/min)\n",
      "2022-05-22 09:54:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://gateway.gofundme.com/web-gateway/v1/feed/help-us-bring-miann-home/counts>: HTTP status code is not handled or not allowed\n",
      "2022-05-22 09:55:11 [scrapy.extensions.logstats] INFO: Crawled 756 pages (at 292 pages/min), scraped 25 items (at 18 items/min)\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "!echo =============== PROJC =============== | tee -a \"./logs/{today}.log\"\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [P] Starting project crawler | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "!scrapy crawl project_spider -s JOBDIR=./crawls/project_spider/ -O ./projects/daily/{today}.json 2>&1 3>&1 | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [P] Finished project crawler | tee -a \"./logs/{today}.log\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1652352542018,
     "user": {
      "displayName": "Zhiyuan DING",
      "userId": "11658084244219590831"
     },
     "user_tz": -60
    },
    "id": "FT-u2N45g4ep",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-May-2022-20:54:44 [CRAWLER] [G] Checking, crawled 7120 out of 7223\n",
      "23-May-2022-20:55:00 [CRAWLER] [G] Uploaded to Google Drive\n",
      "================ END ================\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./crawls\n",
    "import json\n",
    "import os\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "url_db_file = json.load(open('./url/url_db.json', 'r'))\n",
    "daily_projects =json.load(open('./projects/daily/' + today + '.json'))\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [G] Checking, crawled {len(daily_projects)} out of {len(url_db_file)} | tee -a \"./logs/{today}.log\"\n",
    "#today = \"2000-00-00\"\n",
    "\n",
    "!\\cp {'./projects/daily/' + today + '.json'} ~/gdrive/tutorial/projects/daily/\n",
    "!\\cp {'./url/url_db.json'} ~/gdrive/tutorial/url/\n",
    "!\\cp {'./url/daily/' + today + '.json'} ~/gdrive/tutorial/url/daily/\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [G] Uploaded to Google Drive | tee -a \"./logs/{today}.log\"\n",
    "!echo ================ END ================ | tee -a \"./logs/{today}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9ZfHnvsZZV4q"
   ],
   "name": "starter.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
